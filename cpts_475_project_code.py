# -*- coding: utf-8 -*-
"""CptS_475_Project_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D70LRt9LbjOr1htWX49skvj7dNd5uN6r
"""

# Filter data based on accuracy feature
def filterDataAccuracy(data):
  # Only use data that has an error of 100 meters or less
  return data[data['accuracy'] <= 100]

from datetime import datetime

# Convert datetime to standardized time
def convertToStandardizedTime(timeString):
  dt = datetime.strptime(timeString, '%Y-%m-%d %H:%M:%S')
  return int(dt.timestamp())

# Create a new feature yearAndMonth
# Feature used to group data by specific months
def createYearAndMonth(data):
  data['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')
  data['yearAndMonth'] = data['datetime'].dt.to_period('M').astype(str)
  return data

import pandas as pd
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive/')

# Read and load dataset csv file from Google Drive
df = pd.read_csv("/content/drive/MyDrive/all_locations.csv")

# Preprcoessing
# Filter inaccurate datas
df = filterDataAccuracy(df)
# Create standardizedTime feature
df['standardizedTime'] = df['datetime'].apply(convertToStandardizedTime)
# Create YearAndMonth feature
df = createYearAndMonth(df)

import plotly.express as px
import pandas as pd

# Map preprocessed data before using clustering algorithms
fig = px.scatter_mapbox(df,
                        lat="latitude",
                        lon="longitude",
                        hover_data="datetime",
                        zoom=8,
                        height=800,
                        width=800)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

"""## DBSCAN Clustering ##"""

from sklearn.cluster import DBSCAN
import numpy as np

# DBSCAN Clustering Algorithm to cluster geospatial data
def DBSCAN_Clustering(data):
  # Distance threshold of 50 meters (divided by Earth's radius to get eps)
  distance_threshold = 50 / 6371000
  coordinates = data[['latitude', 'longitude']].values
  # Using min of 5 samples per cluster
  db = DBSCAN(eps=distance_threshold, min_samples=5, metric='haversine').fit(np.radians(coordinates))
  data['cluster'] = db.labels_
  return data

df_DBSCAN = DBSCAN_Clustering(df)

# Clean data of any points set as cluster -1 (data points that couldn't meet the min_samples)
df_DBSCAN = df_DBSCAN[df_DBSCAN['cluster'] != -1]

# Map DBSCAN Clusters
fig = px.scatter_mapbox(df_DBSCAN,
                        lat="latitude",
                        lon="longitude",
                        hover_name="cluster",
                        hover_data="datetime",
                        color="cluster",
                        zoom=8,
                        height=800,
                        width=800)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

# Identify top 5 location with DBSCAN Data

# Sort by standardizedTime
df_DBSCAN = df_DBSCAN.sort_values(by='standardizedTime', ascending=True, inplace=False)

# Create timeSpent column for the amount of time spent at each location entry
# ASSUMPTION: Indivudal spends time at same location until next location entry.
df_DBSCAN['timeSpent'] = df_DBSCAN['standardizedTime'].shift(-1) - df_DBSCAN['standardizedTime']

# Fill NaNs
df_DBSCAN['timeSpent'] = df_DBSCAN['timeSpent'].fillna(0)

# groupBy yearAndMonth feature to explore each unique month's top 5 locations
# and groupBy cluster to find amount of time spent at a cluster
time_spent_by_cluster = df_DBSCAN.groupby(['yearAndMonth', 'cluster'])['timeSpent'].sum().reset_index()

# Sort by yearAndMonth feature and timeSpent to get top clusters for each month
top_clusters_per_month = time_spent_by_cluster.sort_values(by=['yearAndMonth', 'timeSpent'], ascending=[True, False]) \
                                               .groupby('yearAndMonth').head(5)

# Save top 5's in a csv file
top_clusters_per_month.to_csv('Top_5_DBSCAN.csv', index=False)

# Calculate total time spent at each cluster
total_time_per_cluster = df_DBSCAN.groupby('cluster')['timeSpent'].sum().reset_index()
total_time_per_cluster = total_time_per_cluster.sort_values(by='timeSpent', ascending=False, inplace=False)

# Save total time spent per cluster in a csv file
total_time_per_cluster.to_csv('Time_Spent_At_Location_DBSCAN.csv', index=False)

# Filter clusters based on total time
import pandas as pd

# Set the threshold for total time
time_threshold = 100000

# Filter clusters based on total time
filtered_clusters = total_time_per_cluster[total_time_per_cluster['timeSpent'] > time_threshold]

# Filter the main DataFrame by keeping only the rows where cluster is in the filtered clusters
df_DBSCAN_filtered = df_DBSCAN[df_DBSCAN['cluster'].isin(filtered_clusters['cluster'])]

# Create the nextCluster column
df_DBSCAN_filtered['nextCluster'] = df_DBSCAN_filtered['cluster'].shift(-1)

df_DBSCAN_filtered['timeSpent'] = df_DBSCAN_filtered['timeSpent'].fillna(-1)

df_DBSCAN_filtered['nextCluster'] = df_DBSCAN_filtered.apply(
    lambda row: -1 if row['cluster'] == row['nextCluster'] else row['nextCluster'],
    axis=1
)

df_DBSCAN_filtered = df_DBSCAN_filtered.iloc[:-1]

# Create directed graph to represent movement patterns
import networkx as nx
import matplotlib.pyplot as plt

# Filter rows where nextCluster is valid (not -1)
valid_edges = df_DBSCAN_filtered[df_DBSCAN_filtered['nextCluster'] != -1]

# Create a directed graph
G = nx.DiGraph()

# Add edges to the graph
for _, row in valid_edges.iterrows():
    G.add_edge(row['cluster'], row['nextCluster'])

# Draw the graph
plt.figure(figsize=(10, 6))
pos = nx.spring_layout(G)  # Positions for all nodes
nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=2000, font_size=12, font_weight='bold')
plt.title("Directed Graph of Clusters")
plt.show()

"""#K-Means Clustering And Folder Saving #"""

from scipy.cluster.vq import whiten, kmeans, vq, kmeans2
from sklearn.cluster import KMeans

kmeans = KMeans(
    init="random",
    n_clusters=16,
    n_init=10,
    max_iter=300,
    random_state=42
)

kmeans.fit(df[["latitude", "longitude"]])
df_kmeans = df
df_kmeans['clusterNumber'] = kmeans.labels_

# Identify top 5 location with K-Means Data
# df = holder
# Sort by standardizedTime
df_kmeans = df_kmeans.sort_values(by='standardizedTime', ascending=True, inplace=False)

# Create timeSpent column for the amount of time spent at each location entry
# ASSUMPTION: Indivudal spends time at same location until next location entry.
df_kmeans['timeSpent'] = df_kmeans['standardizedTime'].shift(-1) - df_kmeans['standardizedTime']

# Fill NaNs
df_kmeans['timeSpent'] = df_kmeans['timeSpent'].fillna(0)

# groupBy yearAndMonth feature to explore each unique month's top 5 locations
# and groupBy cluster to find amount of time spent at a cluster
time_spent_by_cluster = df_kmeans.groupby(['yearAndMonth', 'clusterNumber'])['timeSpent'].sum().reset_index()

# Sort by yearAndMonth feature and timeSpent to get top clusters for each month
top_clusters_per_month = time_spent_by_cluster.sort_values(by=['yearAndMonth', 'timeSpent'], ascending=[True, False]) \
                                               .groupby('yearAndMonth').head(5)
# Save top 5's in a csv file
top_clusters_per_month.to_csv('Top_5_KMeans.csv', index=False)

# Calculate total time spent at each cluster
total_time_per_cluster = df_kmeans.groupby('clusterNumber')['timeSpent'].sum().reset_index()
total_time_per_cluster = total_time_per_cluster.sort_values(by='timeSpent', ascending=False, inplace=False)

# Save total time spent per cluster in a csv file
total_time_per_cluster.to_csv('Time_Spent_At_Location_Kmeans.csv', index=False)

# Map DBSCAN Clusters
fig = px.scatter_mapbox(df_kmeans,
                        lat="latitude",
                        lon="longitude",
                        hover_name="clusterNumber",
                        hover_data="datetime",
                        color="clusterNumber",
                        zoom=8,
                        height=800,
                        width=800)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

# Create the nextCluster column
df_DBSCAN_filtered = df
df_DBSCAN = df
df_DBSCAN_filtered['nextCluster'] = df_DBSCAN_filtered['clusterNumber'].shift(-1)

df_DBSCAN['timeSpent'] = df_DBSCAN['standardizedTime'].shift(-1) - df_DBSCAN['standardizedTime']
df_DBSCAN_filtered['timeSpent'] = df_DBSCAN_filtered['timeSpent'].fillna(-1)

df_DBSCAN_filtered['nextCluster'] = df_DBSCAN_filtered.apply(
    lambda row: -1 if row['clusterNumber'] == row['nextCluster'] else row['nextCluster'],
    axis=1
)

df_DBSCAN_filtered = df_DBSCAN_filtered.iloc[:-1]

# Create directed graph to represent movement patterns
import networkx as nx
import matplotlib.pyplot as plt

# Filter rows where nextCluster is valid (not -1)
valid_edges = df_DBSCAN_filtered[df_DBSCAN_filtered['nextCluster'] != -1]

# Create a directed graph
G = nx.DiGraph()

# Add edges to the graph
for _, row in valid_edges.iterrows():
    G.add_edge(row['clusterNumber'], row['nextCluster'])

# Draw the graph
plt.figure(figsize=(10, 6))
pos = nx.spring_layout(G)  # Positions for all nodes
nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=2000, font_size=12, font_weight='bold')
plt.title("Directed Graph of Clusters")
plt.show()

"""# Plots"""

import pandas as pd
from google.colab import drive

# Read .csv files we generated
Top_5_DBSCAN = pd.read_csv("/content/drive/MyDrive/Top_5_DBSCAN.csv")
Top_5_KMeans = pd.read_csv("/content/drive/MyDrive/Top_5_KMeans.csv")
Time_Spent_At_Location_DBSCAN = pd.read_csv("/content/drive/MyDrive/Time_Spent_At_Location_DBSCAN.csv")
Time_Spent_At_Location_KMeans = pd.read_csv("/content/drive/MyDrive/Time_Spent_At_Location_Kmeans.csv")

import pandas as pd
import matplotlib.pyplot as plt

# Plot top 5 data
# DBSCAN
pivot_data = Top_5_DBSCAN.pivot(index='yearAndMonth', columns='cluster', values='timeSpent')
pivot_data.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')

plt.title('Top 5 Visited Location each Month (DBSCAN)')
plt.xlabel('Month')
plt.ylabel('Total Time (seconds)')
plt.legend(title='Cluster #', loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=5)

# K-Means
pivot_data = Top_5_KMeans.pivot(index='yearAndMonth', columns='clusterNumber', values='timeSpent')
pivot_data.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')

plt.title('Top 5 Visited Location each Month (K-Means)')
plt.xlabel('Month')
plt.ylabel('Total Time (seconds)')
plt.legend(title='Cluster #', loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=5)

# Plot total time data
import matplotlib.pyplot as plt

# DBSCAN
plt.bar(Time_Spent_At_Location_DBSCAN['cluster'], Time_Spent_At_Location_DBSCAN['timeSpent'], color='skyblue')
plt.xlabel('Cluster #')
plt.ylabel('Total Time Spent (seconds)')
plt.yscale('log')
plt.title('Total Time Spent per Cluster (DBSCAN)')
plt.show()

# K-Means
plt.bar(Time_Spent_At_Location_KMeans['clusterNumber'], Time_Spent_At_Location_KMeans['timeSpent'], color='skyblue')
plt.xlabel('Cluster #')
plt.ylabel('Total Time Spent (seconds)')
plt.yscale('log')
plt.title('Total Time Spent per Cluster (K-Means)')
plt.show()